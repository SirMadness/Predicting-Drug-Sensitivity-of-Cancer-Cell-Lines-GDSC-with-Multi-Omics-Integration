{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09c9933",
   "metadata": {},
   "source": [
    "# 00_download.ipynb\n",
    "\n",
    "This notebook fetches the **core GDSC data files** needed for the capstone project.\n",
    "\n",
    "It will create the folder structure `data/raw/` relative to the repository root and download:\n",
    "\n",
    "- Drug‐response curves (IC50 / AUC)\n",
    "- Compound annotations (targets, SMILES)\n",
    "- Cell‑line metadata (tissue, sample provenance)\n",
    "\n",
    "❗ **Large omics matrices** (RNA‑seq, DNA‑methylation, somatic mutations) are *not* downloaded automatically because\n",
    "they are multi‑GB files.  Instead, shell snippets to fetch them from the Sanger FTP mirror are provided—you can\n",
    "uncomment/run those if you want the notebook to pull everything for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6de9f",
   "metadata": {},
   "source": [
    "# 0: Load Modulues, define functions and setup script configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f978c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, pathlib, textwrap\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import ssl\n",
    "from ftplib import FTP\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl, pyarrow # required for polars file reading\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf49162",
   "metadata": {},
   "source": [
    "### Define Definitinons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f68748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dest: Path):\n",
    "    if dest.exists():\n",
    "        print(f'{dest.name} already exists, skipping.')\n",
    "        return\n",
    "    print(f'Downloading {dest.name} …')\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print('✔ Done')\n",
    "\n",
    "def read_parque_to_pd_df(file_path):\n",
    "    \"read parque with polars and then convert to pandas for speed\"\n",
    "    if file_path.exists():\n",
    "        dl_out = pl.read_parquet(file_path)\n",
    "        print(\"Loaded dataframe shape:\",dl_out.shape)\n",
    "        df_out = dl_out.to_pandas()\n",
    "        return df_out\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\\nPlease run the methylation ingestion and transpose step first.\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a1777",
   "metadata": {},
   "source": [
    "## Setup working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd84590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory: /Users/brianr/repos/UCBAIML-GDSC/data/raw\n"
     ]
    }
   ],
   "source": [
    "# Allow TLSv1.2 (required by some older endpoints)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "ROOT = Path.cwd().resolve()  # assumes notebook is run from repo root or subdir\n",
    "if ROOT.name == 'notebooks':\n",
    "    ROOT = ROOT.parent  # go up one level if in notebooks directory\n",
    "\n",
    "DATA_RAW = ROOT / 'data' / 'raw'\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Download directory: {DATA_RAW}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2965f90",
   "metadata": {},
   "source": [
    "## Setup configs for the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf811820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the raw methylation geo file to a data frame WITHOUT COSMID_ID\n",
    "run_meth_ingest_and_transpose = True\n",
    "\n",
    "meth_df_cosmicid_path = DATA_RAW / \"methylation/meth_df_cosmic_ids.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f71e50",
   "metadata": {},
   "source": [
    "# 1: Download Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a77de3",
   "metadata": {},
   "source": [
    "## 1.1: Download GDSC2 (ic50), cell line details, drug data files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4df05b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GDSC2_fitted_dose_response_27Oct23.xlsx': 'https://cog.sanger.ac.uk/cancerrxgene/GDSC_release8.5/GDSC2_fitted_dose_response_27Oct23.xlsx',\n",
       " 'Compounds_Annotation.xlsx': 'https://cog.sanger.ac.uk/cancerrxgene/GDSC_release8.5/screened_compounds_rel_8.5.csv',\n",
       " 'Cell_Lines_Details.xlsx': 'https://cog.sanger.ac.uk/cancerrxgene/GDSC_release8.5/Cell_Lines_Details.xlsx'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = {\n",
    "    'GDSC2_fitted_dose_response_27Oct23.xlsx':\n",
    "        'https://cog.sanger.ac.uk/cancerrxgene/GDSC_release8.5/GDSC2_fitted_dose_response_27Oct23.xlsx',\n",
    "    'Compounds_Annotation.xlsx':\n",
    "        'https://cog.sanger.ac.uk/cancerrxgene/GDSC_release8.5/screened_compounds_rel_8.5.csv',\n",
    "    'Cell_Lines_Details.xlsx':\n",
    "        'https://cog.sanger.ac.uk/cancerrxgene/GDSC_release8.5/Cell_Lines_Details.xlsx',\n",
    "}\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109714da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDSC2_fitted_dose_response_27Oct23.xlsx already exists, skipping.\n",
      "Compounds_Annotation.xlsx already exists, skipping.\n",
      "Cell_Lines_Details.xlsx already exists, skipping.\n"
     ]
    }
   ],
   "source": [
    "# download files\n",
    "for fname, url in files.items():\n",
    "    download(url, DATA_RAW / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dba836",
   "metadata": {},
   "source": [
    "## 1.2: Downloading (GSE) large omics matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76e0969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists!\n"
     ]
    }
   ],
   "source": [
    "## Download methylation data\n",
    "dest = pathlib.Path(\"data/raw/methylation\")\n",
    "dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cmd = textwrap.dedent(f\"\"\"\n",
    "    wget -c \\\\\n",
    "         https://ftp.ncbi.nlm.nih.gov/geo/series/GSE68nnn/GSE68379/suppl/GSE68379_Matrix.processed.txt.gz \\\\\n",
    "         -P {dest}\n",
    "\"\"\").split()\n",
    "local = Path(dest) / Path('GSE68379_Matrix.processed.txt.gz')\n",
    "if not local.exists():\n",
    "    subprocess.run(cmd, check=True)\n",
    "else:\n",
    "    print('File already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95c234",
   "metadata": {},
   "source": [
    "## 1.3: Download (FTP) Variant and Gene Expressin data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc9246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw/gene_expression\n",
      "sanger1018_brainarray_ensemblgene_rma.txt.gz already present\n",
      "data/raw/variants\n",
      "WES_variants.xlsx already present\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# FTP download of variant and gene expression data sets\n",
    "#-- NOTE - if this errors, it is likely do to poor connection, so try again.\n",
    "\n",
    "TARGETS = [\n",
    "    # (remote path, local folder)\n",
    "    ('/pub/project/cancerrxgene/releases/release-7.0/sanger1018_brainarray_ensemblgene_rma.txt.gz',\n",
    "     'data/raw/gene_expression'),\n",
    "    ('/pub/project/cancerrxgene/releases/release-7.0/WES_variants.xlsx', \n",
    "     'data/raw/variants'),\n",
    "]\n",
    "\n",
    "# Maximum number of retries for FTP connection (sometime the Sanger site is down)\n",
    "max_retries = 5\n",
    "\n",
    "for attempt in range(1, max_retries + 1):\n",
    "    try:\n",
    "        with FTP('ftp.sanger.ac.uk') as ftp:\n",
    "            ftp.login()\n",
    "            for remote, local_dir in TARGETS:\n",
    "                print(local_dir)\n",
    "                local = Path(local_dir) / Path(remote).name\n",
    "                local.parent.mkdir(parents=True, exist_ok=True)\n",
    "                if local.exists():\n",
    "                    print(f\"{local.name} already present\")\n",
    "                    continue\n",
    "                with open(local, 'wb') as fh:\n",
    "                    print(f\"⬇ {remote}\")\n",
    "                    ftp.retrbinary(f'RETR {remote}', fh.write)\n",
    "        break  # Success, exit loop\n",
    "    except Exception as e:\n",
    "        print(f\"Sanger site not accessible, try again! Attempt {attempt} of {max_retries}\")\n",
    "        if attempt < max_retries:\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            print(\"Failed after 5 attempts:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d92986",
   "metadata": {},
   "source": [
    "# 2.0: Clean up the Geo Methylation data and add COSMIC_ID for joining data sets\n",
    "- remove _AVE_beta from each cell name. Drop _Predictive_PVAL\n",
    "- Transpose data frame to row = cell and column = cg id\n",
    "\n",
    "\n",
    "For GSE68379_Matrix.processed.txt.gz file. The shape of the meth_raw_df data frame read from the file is (485512, 2057), with column names being in the format of: \n",
    "\n",
    "[0] = 'Row.names' containing cg[000000###]\n",
    "\n",
    "[1], [1:1028] = [cell-line id]_AVG.Beta, containing numeric data\n",
    "\n",
    "[1029:2057] = [cell-line id]_Detection.PVal, containing numeric data\n",
    "\n",
    "Need to transform the data frame to get each cell line and the associated data on a row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233bc05",
   "metadata": {},
   "source": [
    "## 2.1: Read methylation data in and build meth_df dataframe with cell id and Average beta data for each probe id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24288fa6",
   "metadata": {},
   "source": [
    "1. Read data in using Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4522685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen chunk size: 50000\n",
      "File read.\n",
      "Process Chunks\n",
      "Processed 10 chunks.\n",
      "Chunking Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The file size is approximately 4 GB. A chunk size of 50,000 rows is a reasonable starting point\n",
    "# to avoid memory issues while processing the file in chunks.\n",
    "# THIS WILL TAKE A FEW MINUTES\n",
    "\n",
    "meth_df_processed_path = DATA_RAW / \"methylation/meth_df_cellID_only.parquet\"\n",
    "\n",
    "if run_meth_ingest_and_transpose:\n",
    "    chunk_size = 50000\n",
    "    print(f\"Chosen chunk size: {chunk_size}\")\n",
    "\n",
    "\n",
    "    # Read the gzipped CSV file in chunks\n",
    "    methylation_chunks = pd.read_csv(\n",
    "        DATA_RAW / \"methylation/GSE68379_Matrix.processed.txt.gz\",\n",
    "        sep=\"\\t\",\n",
    "        chunksize=chunk_size,\n",
    "        low_memory=False\n",
    "    )\n",
    "    print('File read.')\n",
    "\n",
    "    print('Process Chunks')\n",
    "    # This will return an iterator, so no output is expected immediately.\n",
    "    # Process the chunks in subsequent steps.\n",
    "\n",
    "    processed_chunks = [] # List of chunk dataframes \n",
    "\n",
    "    for chunk in methylation_chunks:\n",
    "        # Identify beta columns (excluding the 'Row.names' column)\n",
    "        beta_cols = [col for col in chunk.columns if col.endswith('_AVG.Beta')]\n",
    "\n",
    "        # Extract 'Row.names' and beta columns\n",
    "        beta_chunk = chunk[['Row.names'] + beta_cols].copy()\n",
    "\n",
    "        # Clean column names by removing the _AVG.Beta suffix\n",
    "        beta_chunk.columns = ['Row.names'] + [col[:-9] for col in beta_cols]\n",
    "\n",
    "        # Set 'Row.names' as index\n",
    "        beta_chunk.set_index('Row.names', inplace=True)\n",
    "\n",
    "        # Clean index and column names for grouping\n",
    "        beta_chunk.columns = (\n",
    "            beta_chunk.columns.str.replace(r\"[ .]\", \"-\", regex=True)\n",
    "            .str.upper()\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "        # Group by column names and calculate the mean to handle replicates\n",
    "        beta_chunk = beta_chunk.groupby(beta_chunk.columns, axis=1).mean()\n",
    "\n",
    "        # Transpose the chunk so rows are cell lines and columns are probes\n",
    "        beta_chunk = beta_chunk.T\n",
    "\n",
    "        # Append the processed chunk to the list\n",
    "        processed_chunks.append(beta_chunk)\n",
    "\n",
    "    print(f\"Processed {len(processed_chunks)} chunks.\")\n",
    "    # Display the first few rows of the first processed chunk to verify\n",
    "    if processed_chunks:\n",
    "        #display(processed_chunks[0].head(5))\n",
    "        print('Chunking Done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40a0ab",
   "metadata": {},
   "source": [
    "2. Build meth_df from chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd37d100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final β-matrix shape: (1028, 485512)\n",
      "Saved meth_df_processed to /Users/brianr/repos/UCBAIML-GDSC/data/raw/methylation/meth_df_cellID_only.parquet\n"
     ]
    }
   ],
   "source": [
    "## Build methylation data frame meth_df from chunks\n",
    "\n",
    "if run_meth_ingest_and_transpose:\n",
    "    # make sure every chunk has identical, ordered index = cell id\n",
    "    template_index = processed_chunks[0].index\n",
    "\n",
    "    dfs = []\n",
    "    for i, df in enumerate(processed_chunks):\n",
    "        # safeguard: all chunks must share the same cells in the same order\n",
    "        if not df.index.equals(template_index):\n",
    "            # ① if they have the same cells but different order → reindex\n",
    "            if set(df.index) == set(template_index):\n",
    "                df = df.reindex(template_index)\n",
    "            # ② if cells are missing or extra → fail fast\n",
    "            else:\n",
    "                raise ValueError(f\"Chunk {i} has mismatching cell lines.\")\n",
    "        dfs.append(df.astype(\"float32\"))     # cast early to save RAM\n",
    "\n",
    "    # verify that probe columns are unique across chunks\n",
    "    all_cols = sum((list(d.columns) for d in dfs), [])\n",
    "    duplicates = pd.Series(all_cols).duplicated()\n",
    "\n",
    "    if duplicates.any():\n",
    "        dup_names = pd.Series(all_cols)[duplicates].unique()\n",
    "        raise ValueError(f\"Probe IDs repeated across chunks: {dup_names[:5]}…\")\n",
    "\n",
    "    # column-wise concatenation  (rows = cells, columns = probes)\n",
    "    meth_df = pd.concat(\n",
    "        dfs,\n",
    "        axis=1,               # ← side-by-side\n",
    "        join=\"inner\",         # identical index already guaranteed\n",
    "        verify_integrity=True # double-checks no duplicate columns slipped in\n",
    "    )\n",
    "\n",
    "    print(\"Final β-matrix shape:\", meth_df.shape)   # (n_cells × n_probes)\n",
    "\n",
    "    # Save meth_df to a compressed parquet file\n",
    "    meth_df.to_parquet(meth_df_processed_path, compression=\"gzip\")\n",
    "    print(f\"Saved meth_df_processed to {meth_df_processed_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66d1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meth_df already exists, skipping ingestion and transposition.\n"
     ]
    }
   ],
   "source": [
    "# read in existing meth_df file\n",
    "if 'meth_df' in locals() and not meth_df.empty:\n",
    "    run_meth_ingest_and_transpose = False\n",
    "    print(\"meth_df already exists, skipping ingestion and transposition.\")\n",
    "else:\n",
    "    run_meth_ingest_and_transpose = True\n",
    "\n",
    "if run_meth_ingest_and_transpose:\n",
    "    # Read the processed file back into a DataFrame\n",
    "    print(\"Read meth_df parquet file\")\n",
    "    meth_df = read_parque_to_pd_df(meth_df_processed_path)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c911dd6",
   "metadata": {},
   "source": [
    "## 2.3: Add Comsic_ID to meth_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add comsic_id to meth_df\n",
    "\n",
    "# 1 ▸  build a lookup   {CELL_LINE_NAME → COSMIC_ID}\n",
    "cl_meta = pd.read_excel(\"data/raw/Cell_Lines_Details.xlsx\")          # release-8.4\n",
    "name_to_cosmic = (\n",
    "    cl_meta.assign(NAME=lambda d: d[\"Sample Name\"].str.upper())\n",
    "           .set_index(\"NAME\")[\"COSMIC identifier\"]\n",
    "           .to_dict()\n",
    ")\n",
    "\n",
    "# Ensure meth_df has a 'cell_id' column; if not, use the index as cell_id\n",
    "if 'cell_id' not in meth_df.columns:\n",
    "    meth_df['cell_id'] = meth_df.index\n",
    "\n",
    "# Standardize cell_id for matching\n",
    "meth_df['cell_id_upper'] = meth_df['cell_id'].str.upper().str.strip()\n",
    "\n",
    "# Map COSMIC_ID using name_to_cosmic\n",
    "meth_df['COSMIC_ID'] = meth_df['cell_id_upper'].map(name_to_cosmic).astype(int)\n",
    "\n",
    "# Drop helper column\n",
    "meth_df.drop(columns=['cell_id_upper'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1f65e",
   "metadata": {},
   "source": [
    "## 2.4 Save meth_df for use in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved meth_df_processed to /Users/brianr/repos/UCBAIML-GDSC/data/raw/methylation/meth_df_cosmic_ids.parquet\n"
     ]
    }
   ],
   "source": [
    "# Reorder meth_df columns to have 'COSMIC_ID' and 'cell_id' as the first columns\n",
    "cols = meth_df.columns.tolist()\n",
    "first_cols = [col for col in ['COSMIC_ID', 'cell_id'] if col in cols]\n",
    "other_cols = [col for col in cols if col not in first_cols]\n",
    "meth_df = meth_df[first_cols + other_cols]\n",
    "\n",
    "# Save meth_df to a compressed parquet file\n",
    "meth_df.to_parquet(meth_df_cosmicid_path, compression=\"gzip\")\n",
    "print(f\"Saved meth_df_processed to {meth_df_cosmicid_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucbai_gdsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
